# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15-O0MyyPQFcw6jeP0m6ccls7GAZx5Phq
"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn import svm
from sklearn.metrics import accuracy_score

"""Import DataSet"""

parkinsons_data = pd.read_csv('/content/parkinsons (1).csv')

parkinsons_data.head()

parkinsons_data.shape

parkinsons_data.shape

parkinsons_data.info()

"""Check for null values"""

parkinsons_data.isnull().sum()

parkinsons_data.describe()

"""distribution of target variable i.e. status 1 = parkisons 0= healthy"""

parkinsons_data['status'].value_counts()

"""Group Data based on the target variable"""

parkinsons_data.groupby('status').mean()

"""here we can see difference between mean values of people with and without parkinsons

separating Features and Target(status) value
here X is for all other values
and Y is for status values
"""

X = parkinsons_data.drop(columns=['name','status'],axis = 1)
Y = parkinsons_data['status']

print(X)

print(Y)

"""Spliting Data into Test data and training Data"""

X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.2,random_state=2)

"""here we create 4 arrays using train test split test size= 20% and random state is 2 which is identity for randomization of data for random size =4 it will distribute diffretnly

"""

print(X.shape,X_train.shape,X_test.shape)



"""Data Standarization

converting data into similar ranges but the meaning of data won't change
we use standard Scalar for it.
"""

scaler = StandardScaler()

scaler.fit(X_train)

"""now it knows how data look it will further make other data look like X_train"""

X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)



"""Model Training

Support Vector Machine Model
Using svc support vector classifier
"""

model = svm.SVC(kernel = 'linear')

"""Training the svm model with training data"""

model.fit(X_train, Y_train)

"""Model evaluation

Accuracy Score
"""

X_train_prediction = model.predict(X_train)#prediction values
training_data_accuracy = accuracy_score(Y_train, X_train_prediction)#compare predicted values with Y_train values

print ("accuracy socre is ", training_data_accuracy*100)

"""we test our model on test data"""

X_test_prediction = model.predict(X_test)#prediction values
training_data_accuracy = accuracy_score(Y_test, X_test_prediction)#compare predicted values with Y_train values

print ("accuracy socre is ", training_data_accuracy*100)



"""The test and train data should not have huge diff.

Building a prediction system
"""

input_data=(119.99200,157.30200,74.99700,0.00784,0.00007,0.00370,0.00554,0.01109,0.04374,0.42600,0.02182,0.03130,0.02971,0.06545,0.02211,21.03300,0.414783,0.815285,-4.813031,0.266482,2.301442,0.284654)#any random data

"""convert touple into numpy array as it is easier and efficient to process"""

input_data_as_numpy_array =np.asarray(input_data)

input_data_reshaped =input_data_as_numpy_array.reshape(1,-1)

"""we have to reshape the data

as our model is expecting 156 valued the values for which it is trained

but here we have only one entry to predict do we have to reshape it
"""

standardized_data = scaler.transform(input_data_reshaped)

prediction = model.predict(standardized_data)

print (prediction)

if(prediction[0]==0):
  print("The person does not have parkinsons disease")
else :
  print("The person does have parkinsons disease")

